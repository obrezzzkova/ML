{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from typing import Dict, Tuple\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Tweets.csv', sep=\",\")\n",
    "data = data[['airline_sentiment', 'text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica What @dhepburn said.',\n",
       " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
       " '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
       " \"@VirginAmerica and it's a really big bad thing about it\",\n",
       " \"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\",\n",
       " '@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)',\n",
       " '@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP',\n",
       " \"@virginamerica Well, I didn't…but NOW I DO! :-D\",\n",
       " \"@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\"]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сформируем общий словарь для обучения моделей из обучающей и тестовой выборки\n",
    "vocab_list = data['text'].tolist()\n",
    "vocab_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 15051\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(vocab_list)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@ VirginAmerica What @ dhepburn said .', \"@ VirginAmerica plus you 've added commercial to the experience ... tacky .\", \"@ VirginAmerica I did n't today ... Must mean I need to take another trip !\", \"@ VirginAmerica it 's really aggressive to blast obnoxious `` entertainment '' in your guest ' face & amp ; they have little recourse\", \"@ VirginAmerica and it 's a really big bad thing about it\", \"@ VirginAmerica seriously would pay $ 30 a flight for seat that did n't have this playing . it 's really the only bad thing about flying VA\", '@ VirginAmerica yes , nearly every time I fly VX this “ ear worm ” won ’ t go away : )', '@ VirginAmerica Really missed a prime opportunity for Men Without Hats parody , there . http : //t.co/mWpG7grEZP', \"@ virginamerica Well , I didn't…but NOW I DO ! : -D\", \"@ VirginAmerica it wa amazing , and arrived an hour early . You 're too good to me .\"]\n"
     ]
    }
   ],
   "source": [
    "#лемматизация\n",
    "vocab_list1 = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(vocab_list)):\n",
    "    word_list = word_tokenize(vocab_list[i])\n",
    "    output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    vocab_list1.append(output)\n",
    "print(vocab_list1[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков после лемматизации- 14423\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(vocab_list1)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков после лемматизации- {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@ virginamerica what @ dhepburn said .', \"@ virginamerica plu you 've ad commerci to the experi ... tacki .\", \"@ virginamerica I did n't today ... must mean I need to take anoth trip !\", \"@ virginamerica it 's realli aggress to blast obnoxi `` entertain '' in your guest ' face & amp ; they have littl recours\", \"@ virginamerica and it 's a realli big bad thing about it\", \"@ virginamerica serious would pay $ 30 a flight for seat that did n't have thi play . it 's realli the onli bad thing about fli VA\", '@ virginamerica ye , nearli everi time I fli VX thi “ ear worm ” won ’ t go away : )', '@ virginamerica realli miss a prime opportun for men without hat parodi , there . http : //t.co/mwpg7grezp', \"@ virginamerica well , I didn't…but now I DO ! : -D\", \"@ virginamerica it wa amaz , and arriv an hour earli . you 're too good to me .\"]\n"
     ]
    }
   ],
   "source": [
    "#стемминг\n",
    "vocab_list2 = []\n",
    "ps = PorterStemmer() \n",
    "for i in range(len(vocab_list)):\n",
    "    word_list = word_tokenize(vocab_list[i])\n",
    "    output = ' '.join([ps.stem(w) for w in word_list])\n",
    "    vocab_list2.append(output)\n",
    "print(vocab_list2[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков после стемминга- 12575\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(vocab_list2)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков после стемминга- {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков после стемминга и удаления стоп-слов- 12526\n"
     ]
    }
   ],
   "source": [
    "#удаление стоп-слов\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vocab_list3 = []\n",
    "for i in range(len(vocab_list2)):\n",
    "    word_list = word_tokenize(vocab_list2[i].lower())\n",
    "    output = ' '.join([word for word in word_list if word not in stop_words])\n",
    "    vocab_list3.append(output) \n",
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(vocab_list3)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков после стемминга и удаления стоп-слов- {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи анализа тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['airline_sentiment'], test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t\\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\пк\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t\t Accuracy\n",
      "negative \t 0.8730744196137991\n",
      "neutral \t 0.5811855670103093\n",
      "positive \t 0.6591889559965487\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t\t Accuracy\n",
      "negative \t 0.9687567802126275\n",
      "neutral \t 0.3382731958762887\n",
      "positive \t 0.4823123382226057\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t\t Accuracy\n",
      "negative \t 0.9264482534172271\n",
      "neutral \t 0.42783505154639173\n",
      "positive \t 0.6471095772217429\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), ComplementNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t\t Accuracy\n",
      "negative \t 0.9681058798003905\n",
      "neutral \t 0.3627577319587629\n",
      "positive \t 0.4089732528041415\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Все методы довольно точно определяют негативные комментарии, однако определение нейтральных и позитивных проблематично.\n",
    "#Однозначно лидирующего метода не выявлено\n",
    "#Наилучшие результаты показали сочетание CountVectorizer + CNB, а также CountVectorizer + LogisticRegression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
